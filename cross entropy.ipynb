{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networ (RNN)\n",
    "\n",
    "a simple RNN model that maps an imput sequence of x words to a corresponding sequence of output o words. We need to notice that the output o is unnormailsed log probabilities. Later in the Loss function, softmax would be use to normalize the output o and cross entropy would be used to measuer how far each o is from corresponding training target y. \n",
    "\n",
    "$$L := \\sum_{t}CE ( y^{(t)}, \\tilde{y}^{(t)} )$$\n",
    "$$\\tilde{y}^{(t)} = softmax(o_{(t)})$$\n",
    "refer to : https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    return exp_X / sum(exp_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function based on loss entropy \n",
    "we should also notice here, for the classification problem, $\\tilde{y}^{(t)}$ might correspond to a vector of probability for each class, and thus  $y^{(t)}$ is a column vector with same dimension and has 1 for the element with index indicating the ture class and 0 otherwise ($y^{(t)}$ one hot encoded vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y,O):\n",
    "    \"\"\"\n",
    "    O is the full output matrix with dimension num_input * num_class\n",
    "    Y is the full output matrix with dimension num_input * num_class\n",
    "    \"\"\"\n",
    "    num_input = Y.shape[0]\n",
    "    num_class = Y.shape[1]\n",
    "    \n",
    "    index = Y.argmax(axis=1) # get the index where y is 1, otherwise the product would 0* no contribution to the sum\n",
    "    \n",
    "    tilde_Y = softmax(O) # compute the softmax of tht output\n",
    "    \n",
    "    log_likelihood = -np.log(p[range(num_input),y])\n",
    "    \n",
    "    return log_likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
